Assume that scalar-real y and two-dimensional real vector x are related to each other according
to y = c(x,w) +v, where c(.,w) is a cubic polynomial in x with coefficients w and v is a random
Gaussian random scalar with mean zero and σ2-variance.
Given a dataset D = (x1, y1),...,(xN, yN) with N samples of (x, y) pairs, with the assumption
that these samples are independent and identically distributed according to the model, derive two
estimators for w using maximum-likelihood (ML) and maximum-a-posteriori (MAP) parameter
estimation approaches as a function of these data samples. For the MAP estimator, assume that w
has a zero-mean Gaussian prior with covariance matrix γI.
Having derived the estimator expressions, implement them in code and apply to the dataset
generated by the attached Matlab script. Using the training dataset, obtain the ML estimator and
the MAP estimator for a variety of γ values ranging from 10−4 to 104. 
Evaluate each trained model by calculating the average-squared error between the y values in the validation samples and
model estimates of these using c(.,wtrained). How does your MAP-trained model perform on the
validation set as γ is varied? How is the MAP estimate related to the ML estimate? Describe your
experiments, visualize and quantify your analyses (e.g. average squared error on validation dataset
as a function of hyperparameter γ) with data from these experiments.
